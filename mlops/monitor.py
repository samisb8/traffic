from datetime import datetime, timedelta
from pathlib import Path

import mlflow
import requests


def check_data_drift():
    """DÃ©tecte le drift dans les donnÃ©es SANS MLflow tracking imbriquÃ©"""
    print("ğŸ“Š VÃ©rification drift des donnÃ©es...")

    # DonnÃ©es d'entraÃ®nement (rÃ©fÃ©rence)
    train_data_path = Path("data/traffic_data.csv")
    if not train_data_path.exists():
        print("âš ï¸ DonnÃ©es d'entraÃ®nement non trouvÃ©es")
        return {"drift_score": 0.0, "status": "unknown"}

    train_data = pd.read_csv(train_data_path)

    # Simulation donnÃ©es rÃ©centes (dans un vrai cas: API/DB)
    recent_data = generate_recent_data()

    # Calcul drift simple (diffÃ©rence moyennes)
    drift_scores = {}
    features = ["hour", "day_of_week", "weather_score", "event_impact"]

    for feature in features:
        if feature in train_data.columns and feature in recent_data.columns:
            train_mean = train_data[feature].mean()
            recent_mean = recent_data[feature].mean()
            if train_mean != 0:
                drift_scores[feature] = abs(train_mean - recent_mean) / abs(train_mean)
            else:
                drift_scores[feature] = 0.0

    overall_drift = np.mean(list(drift_scores.values())) if drift_scores else 0.0

    # Seuil d'alerte
    drift_threshold = 0.1
    status = "ok" if overall_drift < drift_threshold else "alert"

    print(
        f"ğŸ“ˆ Drift score: {overall_drift:.4f} ({'âš ï¸ ALERT' if status == 'alert' else 'âœ… OK'})"
    )

    return {
        "drift_score": round(overall_drift, 4),
        "feature_drifts": {k: round(v, 4) for k, v in drift_scores.items()},
        "status": status,
        "threshold": drift_threshold,
        "timestamp": datetime.now().isoformat(),
    }


def generate_recent_data():
    """GÃ©nÃ¨re donnÃ©es rÃ©centes simulÃ©es"""
    np.random.seed(int(datetime.now().timestamp()) % 1000)
    n_samples = 100

    # LÃ©gÃ¨re dÃ©rive simulÃ©e
    drift_factor = 1.1

    return pd.DataFrame(
        {
            "hour": np.random.randint(0, 24, n_samples),
            "day_of_week": np.random.randint(0, 7, n_samples),
            "weather_score": np.random.uniform(0, 1, n_samples) * drift_factor,
            "event_impact": np.random.uniform(0, 1, n_samples),
            "historical_avg": np.random.uniform(0.3, 0.9, n_samples),
        }
    )


def check_model_performance():
    """VÃ©rifie performance du modÃ¨le en production SANS MLflow tracking imbriquÃ©"""
    print("ğŸ¤– VÃ©rification performance modÃ¨le...")

    try:
        # MÃ©triques depuis l'API
        response = requests.get("http://localhost:8000/metrics", timeout=5)
        if response.status_code == 200:
            api_metrics = response.json()
            print("âœ… MÃ©triques rÃ©cupÃ©rÃ©es depuis l'API")
            return api_metrics
    except Exception:
        print("âš ï¸ API non accessible, utilisation mÃ©triques locales")

    # Fallback: mÃ©triques locales
    metrics_path = Path("data/model_production_metrics.json")
    if metrics_path.exists():
        with open(metrics_path) as f:
            local_metrics = json.load(f)
            print("âœ… MÃ©triques locales chargÃ©es")
            return local_metrics

    print("âš ï¸ Aucune mÃ©trique disponible")
    return {"error": "Aucune mÃ©trique disponible"}


def generate_monitoring_report():
    """GÃ©nÃ¨re rapport de monitoring complet avec UN SEUL run MLflow"""
    print("ğŸ“‹ GÃ©nÃ©ration rapport monitoring...")

    # Forcer la fermeture de tous les runs
    try:
        while mlflow.active_run():
            mlflow.end_run()
    except Exception:
        pass

    # S'assurer qu'on est dans la bonne expÃ©rience
    mlflow.set_experiment("Casablanca_Traffic_Monitoring")

    # Collecte des donnÃ©es AVANT de dÃ©marrer MLflow
    print("ğŸ” Collecte des donnÃ©es de drift...")
    drift_info = check_data_drift()

    print("ğŸ” Collecte des donnÃ©es de performance...")
    model_perf = check_model_performance()

    # GÃ©nÃ©ration du rapport
    report = {
        "timestamp": datetime.now().isoformat(),
        "data_drift": drift_info,
        "model_performance": model_perf,
        "recommendations": [],
    }

    # Recommandations automatiques
    if drift_info.get("status") == "alert":
        report["recommendations"].append(
            "ğŸ”„ Re-entraÃ®nement recommandÃ© (drift dÃ©tectÃ©)"
        )

    if isinstance(model_perf, dict):
        # VÃ©rifier si on a des mÃ©triques de modÃ¨le
        accuracy = None
        if "model_metrics" in model_perf:
            accuracy = model_perf["model_metrics"].get("accuracy", 0)
        elif "accuracy" in model_perf:
            accuracy = model_perf.get("accuracy", 0)

        if accuracy and accuracy < 0.8:
            report["recommendations"].append("ğŸ“‰ Performance dÃ©gradÃ©e, vÃ©rifier modÃ¨le")

    if not report["recommendations"]:
        report["recommendations"].append("âœ… SystÃ¨me stable, aucune action requise")

    # Sauvegarde rapport
    report_path = Path("data/monitoring_report.json")
    report_path.parent.mkdir(exist_ok=True)

    with open(report_path, "w") as f:
        json.dump(report, f, indent=2)

    print(f"ğŸ“‹ Rapport sauvÃ©: {report_path}")

    # MAINTENANT on log tout dans MLflow avec UN SEUL run
    try:
        with mlflow.start_run(
            run_name=f"monitoring_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        ):
            # Log toutes les mÃ©triques drift
            mlflow.log_metric("overall_drift_score", drift_info.get("drift_score", 0.0))
            for feature, score in drift_info.get("feature_drifts", {}).items():
                mlflow.log_metric(f"drift_{feature}", score)

            # Log mÃ©triques de performance si disponibles
            if isinstance(model_perf, dict) and "accuracy" in model_perf:
                mlflow.log_metric("model_accuracy", model_perf["accuracy"])

            # Log paramÃ¨tres
            mlflow.log_param("drift_threshold", drift_info.get("threshold", 0.1))
            mlflow.log_param("drift_status", drift_info.get("status", "unknown"))
            mlflow.log_param("recommendations", ", ".join(report["recommendations"]))
            mlflow.log_param("monitoring_type", "full_report")
            mlflow.log_param("timestamp", datetime.now().isoformat())

            # Log du rapport comme artefact
            mlflow.log_artifact(str(report_path))

            print("âœ… Rapport enregistrÃ© dans MLflow")

    except Exception as e:
        print(f"âš ï¸ Erreur MLflow: {e}")
        print("ğŸ“‹ Rapport sauvÃ© localement quand mÃªme")

    # Affichage rÃ©sumÃ©
    print("\nğŸ“Š RÃ‰SUMÃ‰ MONITORING:")
    print(f"   Drift Score: {drift_info.get('drift_score', 'N/A')}")
    print(f"   Status: {drift_info.get('status', 'N/A')}")
    for rec in report["recommendations"]:
        print(f"   {rec}")

    return report


if __name__ == "__main__":
    # Nettoyage radical au dÃ©but
    try:
        while mlflow.active_run():
            mlflow.end_run()
    except Exception:
        pass

    # ExÃ©cution du monitoring
    try:
        report = generate_monitoring_report()
        print("ğŸ‰ Monitoring terminÃ© avec succÃ¨s!")
    except Exception as e:
        print(f"ğŸ’¥ Erreur lors du monitoring: {e}")
        print("ğŸ”„ RedÃ©marrez MLflow si le problÃ¨me persiste")

from datetime import datetime, timedelta
from pathlib import Path

import mlflow
import numpy as np
import pandas as pd
import requests
from streamlit import json


def check_data_drift():
    """D√©tecte le drift dans les donn√©es SANS MLflow tracking imbriqu√©"""
    print("üìä V√©rification drift des donn√©es...")

    # Donn√©es d'entra√Ænement (r√©f√©rence)
    train_data_path = Path("data/traffic_data.csv")
    if not train_data_path.exists():
        print("‚ö†Ô∏è Donn√©es d'entra√Ænement non trouv√©es")
        return {"drift_score": 0.0, "status": "unknown"}

    train_data = pd.read_csv(train_data_path)

    # Simulation donn√©es r√©centes (dans un vrai cas: API/DB)
    recent_data = generate_recent_data()

    # Calcul drift simple (diff√©rence moyennes)
    drift_scores = {}
    features = ["hour", "day_of_week", "weather_score", "event_impact"]

    for feature in features:
        if feature in train_data.columns and feature in recent_data.columns:
            train_mean = train_data[feature].mean()
            recent_mean = recent_data[feature].mean()
            if train_mean != 0:
                drift_scores[feature] = abs(train_mean - recent_mean) / abs(train_mean)
            else:
                drift_scores[feature] = 0.0

    overall_drift = np.mean(list(drift_scores.values())) if drift_scores else 0.0

    # Seuil d'alerte
    drift_threshold = 0.1
    status = "ok" if overall_drift < drift_threshold else "alert"

    print(
        f"üìà Drift score: {overall_drift:.4f} ({'‚ö†Ô∏è ALERT' if status == 'alert' else '‚úÖ OK'})"
    )

    return {
        "drift_score": round(overall_drift, 4),
        "feature_drifts": {k: round(v, 4) for k, v in drift_scores.items()},
        "status": status,
        "threshold": drift_threshold,
        "timestamp": datetime.now().isoformat(),
    }


def generate_recent_data():
    """G√©n√®re donn√©es r√©centes simul√©es"""
    np.random.seed(int(datetime.now().timestamp()) % 1000)
    n_samples = 100

    # L√©g√®re d√©rive simul√©e
    drift_factor = 1.1

    return pd.DataFrame(
        {
            "hour": np.random.randint(0, 24, n_samples),
            "day_of_week": np.random.randint(0, 7, n_samples),
            "weather_score": np.random.uniform(0, 1, n_samples) * drift_factor,
            "event_impact": np.random.uniform(0, 1, n_samples),
            "historical_avg": np.random.uniform(0.3, 0.9, n_samples),
        }
    )


def check_model_performance():
    """V√©rifie performance du mod√®le en production SANS MLflow tracking imbriqu√©"""
    print("ü§ñ V√©rification performance mod√®le...")

    try:
        # M√©triques depuis l'API
        response = requests.get("http://localhost:8000/metrics", timeout=5)
        if response.status_code == 200:
            api_metrics = response.json()
            print("‚úÖ M√©triques r√©cup√©r√©es depuis l'API")
            return api_metrics
    except Exception:
        print("‚ö†Ô∏è API non accessible, utilisation m√©triques locales")

    # Fallback: m√©triques locales
    metrics_path = Path("data/model_production_metrics.json")
    if metrics_path.exists():
        with open(metrics_path) as f:
            local_metrics = json.load(f)
            print("‚úÖ M√©triques locales charg√©es")
            return local_metrics

    print("‚ö†Ô∏è Aucune m√©trique disponible")
    return {"error": "Aucune m√©trique disponible"}


def generate_monitoring_report():
    """G√©n√®re rapport de monitoring complet avec UN SEUL run MLflow"""
    print("üìã G√©n√©ration rapport monitoring...")

    # Forcer la fermeture de tous les runs
    try:
        while mlflow.active_run():
            mlflow.end_run()
    except Exception:
        pass

    # S'assurer qu'on est dans la bonne exp√©rience
    mlflow.set_experiment("Casablanca_Traffic_Monitoring")

    # Collecte des donn√©es AVANT de d√©marrer MLflow
    print("üîç Collecte des donn√©es de drift...")
    drift_info = check_data_drift()

    print("üîç Collecte des donn√©es de performance...")
    model_perf = check_model_performance()

    # G√©n√©ration du rapport
    report = {
        "timestamp": datetime.now().isoformat(),
        "data_drift": drift_info,
        "model_performance": model_perf,
        "recommendations": [],
    }

    # Recommandations automatiques
    if drift_info.get("status") == "alert":
        report["recommendations"].append(
            "üîÑ Re-entra√Ænement recommand√© (drift d√©tect√©)"
        )

    if isinstance(model_perf, dict):
        # V√©rifier si on a des m√©triques de mod√®le
        accuracy = None
        if "model_metrics" in model_perf:
            accuracy = model_perf["model_metrics"].get("accuracy", 0)
        elif "accuracy" in model_perf:
            accuracy = model_perf.get("accuracy", 0)

        if accuracy and accuracy < 0.8:
            report["recommendations"].append("üìâ Performance d√©grad√©e, v√©rifier mod√®le")

    if not report["recommendations"]:
        report["recommendations"].append("‚úÖ Syst√®me stable, aucune action requise")

    # Sauvegarde rapport
    report_path = Path("data/monitoring_report.json")
    report_path.parent.mkdir(exist_ok=True)

    with open(report_path, "w") as f:
        json.dump(report, f, indent=2)

    print(f"üìã Rapport sauv√©: {report_path}")

    # MAINTENANT on log tout dans MLflow avec UN SEUL run
    try:
        with mlflow.start_run(
            run_name=f"monitoring_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        ):
            # Log toutes les m√©triques drift
            mlflow.log_metric("overall_drift_score", drift_info.get("drift_score", 0.0))
            for feature, score in drift_info.get("feature_drifts", {}).items():
                mlflow.log_metric(f"drift_{feature}", score)

            # Log m√©triques de performance si disponibles
            if isinstance(model_perf, dict) and "accuracy" in model_perf:
                mlflow.log_metric("model_accuracy", model_perf["accuracy"])

            # Log param√®tres
            mlflow.log_param("drift_threshold", drift_info.get("threshold", 0.1))
            mlflow.log_param("drift_status", drift_info.get("status", "unknown"))
            mlflow.log_param("recommendations", ", ".join(report["recommendations"]))
            mlflow.log_param("monitoring_type", "full_report")
            mlflow.log_param("timestamp", datetime.now().isoformat())

            # Log du rapport comme artefact
            mlflow.log_artifact(str(report_path))

            print("‚úÖ Rapport enregistr√© dans MLflow")

    except Exception as e:
        print(f"‚ö†Ô∏è Erreur MLflow: {e}")
        print("üìã Rapport sauv√© localement quand m√™me")

    # Affichage r√©sum√©
    print("\nüìä R√âSUM√â MONITORING:")
    print(f"   Drift Score: {drift_info.get('drift_score', 'N/A')}")
    print(f"   Status: {drift_info.get('status', 'N/A')}")
    for rec in report["recommendations"]:
        print(f"   {rec}")

    return report


if __name__ == "__main__":
    # Nettoyage radical au d√©but
    try:
        while mlflow.active_run():
            mlflow.end_run()
    except Exception:
        pass

    # Ex√©cution du monitoring
    try:
        report = generate_monitoring_report()
        print("üéâ Monitoring termin√© avec succ√®s!")
    except Exception as e:
        print(f"üí• Erreur lors du monitoring: {e}")
        print("üîÑ Red√©marrez MLflow si le probl√®me persiste")
